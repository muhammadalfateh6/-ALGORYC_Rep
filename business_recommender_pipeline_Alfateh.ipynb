{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv('test.csv')\n",
    "df = pd.read_csv('business_data_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580708f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Date': 'Timestamp'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2facb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby([\"Suburb\"]).apply(lambda x: x.sort_values([\"Timestamp\"], ascending = True)).reset_index(drop=True)\n",
    "# df = df.groupby([\"Suburb\"]).apply(lambda x: x.sort_values([\"Timestamp\"], ascending = True)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d2c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['Timestamp'], axis=1)\n",
    "# cols = [col for col in df.columns if col != 'Business category'] + ['Business category']\n",
    "# df = df[cols]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd10b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Timestamp'] == '2023-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0811b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Business Category': 'Business category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ccee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Timestamp'], axis=1)\n",
    "cols = [col for col in df.columns if col != 'Business category'] + ['Business category']\n",
    "df = df[cols]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['Timestamp'], axis=1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['Suburb'] = label_encoder.fit_transform(df['Suburb'])\n",
    "num_suburbs = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089fc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_suburbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405618c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Business category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdee688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Business category'] = label_encoder.fit_transform(df['Business category'])\n",
    "num_categories = len(label_encoder.classes_)\n",
    "num_sequences = len(df) - 300 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba2b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Sample categories\n",
    "# # categories = ['food', 'technology', 'fashion', 'health']\n",
    "# categories = df['Business category'].unique()\n",
    "\n",
    "# # Create OneHotEncoder object\n",
    "# encoder = OneHotEncoder()\n",
    "\n",
    "# # Fit and transform categories\n",
    "# one_hot_encoded = encoder.fit_transform(np.array(categories).reshape(-1, 1))\n",
    "\n",
    "# # Print the resulting one-hot encoded matrix\n",
    "# print(one_hot_encoded.toarray())\n",
    "# print (categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded_data = encoder.inverse_transform(one_hot_encoded)\n",
    "\n",
    "# print(decoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a79140",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df['Business category'])\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded_df = pd.DataFrame({'Decoded': one_hot.idxmax(axis=1)})\n",
    "# print(decoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a130d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.iloc[:,:-1], one_hot], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:300,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bb1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Normalize the numerical columns\n",
    "# df['Suburb'] = (df['Suburb'] - df['Suburb'].min()) / (df['Suburb'].max() - df['Suburb'].min())\n",
    "# df['Population'] = (df['Population'] - df['Population'].min()) / (df['Population'].max() - df['Population'].min())\n",
    "# df['Average income'] = (df['Average income'] - df['Average income'].min()) / (df['Average income'].max() - df['Average income'].min())\n",
    "# df['Average monthly sales'] = (df['Average monthly sales'] - df['Average monthly sales'].min()) / (df['Average monthly sales'].max() - df['Average monthly sales'].min())\n",
    "# df['Average monthly profit'] = (df['Average monthly profit'] - df['Average monthly profit'].min()) / (df['Average monthly profit'].max() - df['Average monthly profit'].min())\n",
    "# df['Number of employees'] = (df['Number of employees'] - df['Number of employees'].min()) / (df['Number of employees'].max() - df['Number of employees'].min())\n",
    "# df['Average spending'] = (df['Average spending'] - df['Average spending'].min()) / (df['Average spending'].max() - df['Average spending'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2131a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e04ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "data1 = pd.read_csv('Processed Data Aus.csv')\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6165ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0215ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data3.groupby([\"suburb_name_encoded\"]).apply(lambda x: x.sort_values([\"time\"], ascending = True)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped1 = grouped.drop(columns=['time','suburb_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2=grouped1.reindex(columns=['user_behavior',\n",
    "                                   'business_performance',\n",
    "                                   'value',\n",
    "                                   'business_reviews',\n",
    "                                   'business_rating',\n",
    "                                   'suburb_name_encoded',\n",
    "                                   'business_category_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82091c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped3 = grouped2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc59865",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3dd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped3[:, :-1].shape, grouped3[:, -1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efb7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into input and output sequences\n",
    "X, y = [], []\n",
    "seq_len = 30\n",
    "# for i in range(len(data) - seq_len):\n",
    "for i in range(0, len(grouped3) - seq_len, seq_len-1):\n",
    "    X.append(grouped3[i:i+seq_len, :-1])\n",
    "    y.append(grouped3[i+seq_len-1, -1:])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Print shapes of input and output arrays\n",
    "print('Input shape:', X.shape)\n",
    "print('Output shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.2 * len(X))\n",
    "test_size = len(X) - train_size - val_size\n",
    "\n",
    "train_X, val_X, test_X = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
    "train_y, val_y, test_y = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n",
    "\n",
    "print(\"Training data shape:\", train_X.shape, train_y.shape)\n",
    "print(\"Validation data shape:\", val_X.shape, val_y.shape)\n",
    "print(\"Test data shape:\", test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39122861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "data = pd.read_csv('Ready_Business_data.csv')\n",
    "data1 = data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5283ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_noise=['business_reviews','business_rating']\n",
    "noise_std= 0.1\n",
    "\n",
    "mask = np.zeros(len(data1), dtype=bool)\n",
    "mask[:14415] = True\n",
    "np.random.shuffle(mask)\n",
    "\n",
    "noise = np.zeros((len(data1), len(cols_to_noise)))\n",
    "noise[mask, :] = np.random.normal(scale=noise_std, size=(mask.sum(), len(cols_to_noise)))\n",
    "data1.loc[mask, cols_to_noise] += noise[mask, :]\n",
    "\n",
    "data1.to_csv('noisy_datav7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100a6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_d=pd.read_csv('noisy_datav7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "390bc0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = noisy_d.drop(columns=['business_category_encoded'])\n",
    "#col = ['business_category_encoded']\n",
    "y = noisy_d['business_category_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9468ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1350474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 16:37:32.835707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 16:37:32.855009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 16:37:32.855215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 16:37:32.856221: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 16:37:32.857929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 16:37:32.858106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 16:37:32.858217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 16:37:33.223931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 16:37:33.224038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 16:37:33.224090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 16:37:33.224337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5870 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(60, input_shape=(6, 1), return_sequences=True), \n",
    "    tf.keras.layers.LSTM(50, return_sequences=False),\n",
    "    tf.keras.layers.Dense(52, activation ='softmax')\n",
    "    ])\n",
    "\n",
    "#callback = [EarlyStopping(monitor='accuracy', patience=30)]\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model1.compile(optimizer = optim,\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722ddb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 16:37:35.488153: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12/1352 [..............................] - ETA: 6s - loss: 3.9495 - accuracy: 0.0257   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 16:37:36.010678: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352/1352 [==============================] - 14s 9ms/step - loss: 3.8802 - accuracy: 0.0365 - val_loss: 3.8119 - val_accuracy: 0.0486\n",
      "Epoch 2/400\n",
      "1352/1352 [==============================] - 12s 9ms/step - loss: 3.6419 - accuracy: 0.0754 - val_loss: 3.4671 - val_accuracy: 0.1060\n",
      "Epoch 3/400\n",
      "1352/1352 [==============================] - 12s 9ms/step - loss: 3.3190 - accuracy: 0.1287 - val_loss: 3.1886 - val_accuracy: 0.1486\n",
      "Epoch 4/400\n",
      "1352/1352 [==============================] - 12s 9ms/step - loss: 3.0591 - accuracy: 0.1711 - val_loss: 2.9446 - val_accuracy: 0.1883\n",
      "Epoch 5/400\n",
      "1352/1352 [==============================] - 12s 9ms/step - loss: 2.8434 - accuracy: 0.2116 - val_loss: 2.7493 - val_accuracy: 0.2338\n",
      "Epoch 6/400\n",
      "1352/1352 [==============================] - 11s 8ms/step - loss: 2.6572 - accuracy: 0.2504 - val_loss: 2.5768 - val_accuracy: 0.2674\n",
      "Epoch 7/400\n",
      "1349/1352 [============================>.] - ETA: 0s - loss: 2.4969 - accuracy: 0.2860"
     ]
    }
   ],
   "source": [
    "history = model1.fit(x_train, y_train, \n",
    "                     validation_split =0.1,\n",
    "                     #callbacks=callback,\n",
    "                     batch_size =256,\n",
    "                     epochs = 400,\n",
    "                     verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88c999",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Training Acc', 'Validation Acc'], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2371fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x_test.values.reshape(x_test.shape[0], x_test.shape[1], 1 )\n",
    "y2 = y_test.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f52f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lstm = model1.predict(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae07383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "print('Avg Accuracy:  ',accuracy_score(pred_lstm, y2,\n",
    "                                       pos_label='positive',average='macro'))\n",
    "print('Avg Precision: ', precision_score(pred_lstm, y2,\n",
    "                                         pos_label='positive',average='macro'))\n",
    "print('Avg Recall:    ',recall_score(pred_lstm, y2,\n",
    "                                     pos_label='positive',average='macro'))\n",
    "print('Avg F1-Score:  ',f1_score(pred_lstm, y2, \n",
    "                                 pos_label='positive', average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x_test.values.reshape(x_test.shape[0], x_test.shape[1], 1 )\n",
    "y2 = y_test.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0208c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred = model1.predict(x2)\n",
    "predicted = np.argmax(pred, axis=1).reshape(-1,1)\n",
    "\n",
    "matrix = confusion_matrix(y2, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predicted, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d9f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdec94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9cf4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Load preprocessed data and split into X and y\n",
    "# # X = np.load('preprocessed_data_X.npy')\n",
    "# # y = np.load('preprocessed_data_y.npy')\n",
    "\n",
    "# # Split into training, validation and test sets\n",
    "# train_size = int(0.7 * len(X))\n",
    "# val_size = int(0.2 * len(X))\n",
    "# test_size = len(X) - train_size - val_size\n",
    "\n",
    "# X_train, y_train, test_X = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
    "# train_y, val_y, test_y = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n",
    "\n",
    "# print(\"Training data shape:\", train_X.shape, train_y.shape)\n",
    "# print(\"Validation data shape:\", val_X.shape, val_y.shape)\n",
    "# print(\"Test data shape:\", test_X.shape, test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02395c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def create_sequences(df, seq_len):\n",
    "#     '''\n",
    "#     Pre-processes data by creating sequences of fixed length\n",
    "\n",
    "#     Parameters:\n",
    "#     df (pd.DataFrame): Input dataframe wi15th business recommendation data\n",
    "#     seq_len (int): Length of each sequence\n",
    "\n",
    "#     Returns:\n",
    "#     X (np.array): Array of input sequences\n",
    "#     y (np.array): Array of output targets\n",
    "#     '''\n",
    "\n",
    "#     # Normalize the numerical columns\n",
    "#     df['Population'] = (df['Population'] - df['Population'].min()) / (df['Population'].max() - df['Population'].min())\n",
    "#     df['Average income'] = (df['Average income'] - df['Average income'].min()) / (df['Average income'].max() - df['Average income'].min())\n",
    "#     df['Average monthly sales'] = (df['Average monthly sales'] - df['Average monthly sales'].min()) / (df['Average monthly sales'].max() - df['Average monthly sales'].min())\n",
    "#     df['Average monthly profit'] = (df['Average monthly profit'] - df['Average monthly profit'].min()) / (df['Average monthly profit'].max() - df['Average monthly profit'].min())\n",
    "#     df['Number of employees'] = (df['Number of employees'] - df['Number of employees'].min()) / (df['Number of employees'].max() - df['Number of employees'].min())\n",
    "#     df['Average spending'] = (df['Average spending'] - df['Average spending'].min()) / (df['Average spending'].max() - df['Average spending'].min())\n",
    "\n",
    "#     # Convert data to numpy arrays\n",
    "#     data = df.to_numpy()\n",
    "\n",
    "#     # Split data into input and output sequences\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(data) - seq_len):\n",
    "#         X.append(data[i:i+seq_len, :-1])\n",
    "#         y.append(data[i+seq_len-1, -1])\n",
    "\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "\n",
    "#     # Print shapes of input and output arrays\n",
    "#     print('Input shape:', X.shape)\n",
    "#     print('Output shape:', y.shape)\n",
    "\n",
    "#     return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f3c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "15# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def create_sequences(df, seq_len):\n",
    "#     '''\n",
    "#     Pre-processes data by creating sequences of fixed length\n",
    "\n",
    "#     Parameters:\n",
    "#     df (pd.DataFrame): Input dataframe with business recommendation data\n",
    "#     seq_len (int): Length of each sequence\n",
    "\n",
    "#     Returns:\n",
    "#     X (np.array): Array of input sequences\n",
    "#     y (np.array): Array of output targets\n",
    "#     '''\n",
    "\n",
    "#     # Drop unnecessary columns\n",
    "#     df = df.drop(['Timestamp'], axis=1)\n",
    "\n",
    "#     # Create dictionary of category to index mapping\n",
    "#     category_to_index = {}\n",
    "#     categories = np.sort(df['Business category'].unique())\n",
    "#     for i, category in enumerate(categories):\n",
    "#         category_to_index[category] = i\n",
    "\n",
    "#     # Replace category strings with corresponding index values\n",
    "#     df['Business category'] = df['Business category'].map(category_to_index)\n",
    "\n",
    "#     # Normalize the numerical columns\n",
    "#     df['Population'] = (df['Population'] - df['Population'].min()) / (df['Population'].max() - df['Population'].min())\n",
    "#     df['Average income'] = (df['Average income'] - df['Average income'].min()) / (df['Average income'].max() - df['Average income'].min())\n",
    "#     df['Average monthly sales'] = (df['Average monthly sales'] - df['Average monthly sales'].min()) / (df['Average monthly sales'].max() - df['Average monthly sales'].min())\n",
    "#     df['Average monthly profit'] = (df['Average monthly profit'] - df['Average monthly profit'].min()) / (df['Average monthly profit'].max() - df['Average monthly profit'].min())\n",
    "#     df['Number of employees'] = (df['Number of employees'] - df['Number of employees'].min()) / (df['Number of employees'].max() - df['Number of employees'].min())\n",
    "#     df['Average spending'] = (df['Average spending'] - df['Average spending'].min()) / (df['Average spending'].max() - df['Average spending'].min())\n",
    "\n",
    "#     # Convert data to numpy arrays\n",
    "#     data = df.to_numpy()\n",
    "\n",
    "#     # Split data into input and output sequences\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(data) - seq_len):\n",
    "#         X.append(data[i:i+seq_len, :-1])\n",
    "#         y.append(data[i+seq_len-1, -1])\n",
    "\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "\n",
    "#     # Print shapes of input and output arrays\n",
    "#     print('Input shape:', X.shape)\n",
    "#     print('Output shape:', y.shape)\n",
    "\n",
    "#     return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728573f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data and split into X and y\n",
    "# X = np.load('preprocessed_data_X.npy')\n",
    "# y = np.load('preprocessed_data_y.npy')\n",
    "\n",
    "# Split into training, validation and test sets\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.1 * len(X))\n",
    "test_size = len(X) - train_size - val_size\n",
    "\n",
    "train_X, val_X, test_X = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
    "train_y, val_y, test_y = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n",
    "\n",
    "print(\"Training data shape:\", train_X.shape, train_y.shape)\n",
    "print(\"Validation data shape:\", val_X.shape, val_y.shape)\n",
    "print(\"Test data shape:\", test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df['Business category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20757394",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape[1], train_X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51efd922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# num_categories = len(df['Business category'].unique())\n",
    "num_categories = 10\n",
    "# Define model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(512, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\"),\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\"),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(32),# activation='relu'),\n",
    "    tf.keras.layers.Dense(16),# activation='relu'),\n",
    "    tf.keras.layers.Dense(num_categories)#, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.utils.plot_model(model, show_shapes=True, dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001)\n",
    "\n",
    "# model.compile(loss='mse', optimizer='adam')\n",
    "model.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "#             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "#callbacks=callbacks, \n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "            ModelCheckpoint(filepath='test_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "start = time()\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=100, validation_data=(val_X, val_y), verbose=1, shuffle=False, \n",
    "                    batch_size=128, callbacks=callbacks)\n",
    "\n",
    "# Early Stopping\n",
    "#keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('training time = ',time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b718bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78336f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pred = np.argmax(pred, axis=1)\n",
    "seq_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9425e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_gt = np.argmax(test_y, axis=1)\n",
    "seq_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeff720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "seq_gt = test_y\n",
    "cm = confusion_matrix(seq_gt,seq_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(seq_gt, seq_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b560828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acf16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile model\n",
    "# # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#     from_logits=False,\n",
    "#     ignore_class=None,\n",
    "#     reduction=\"auto\",\n",
    "#     name=\"sparse_categorical_crossentropy\",\n",
    "# )\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#               loss=loss,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train model\n",
    "# history = model.fit(train_X, train_y, epochs=100, batch_size=16, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70555c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed209d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate model\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=32)\n",
    "# print('Test loss:', test_loss)\n",
    "# print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data with timestamp in a non-standard format\n",
    "data = pd.read_csv('test.csv')\n",
    "print(data)\n",
    "\n",
    "# Convert timestamp to datetime format\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%d-%m-%y %H:%M:%S', errors='coerce')\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed83342",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b008ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
